{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Udacity Data Engineer Nanodegree - Capstone Project\n",
    "\n",
    "## Project summary - \n",
    "    In this project we are building a data warehouse which will integrate the data from 3 datasets, the immigration dataset, global temperature dataset and the us demography dataset. By the final modelled data we can aim find out the visa related information of a person, by which airline he came and if he departed on a particular date or not and how was the wheather on his visit day. Also we can find out which state is most favoured by the immigrants for visiting and what was the population of that state by sex, foreign born and veteran, also in which month or year was the rate of immigration highest.\n",
    "    The output of one such analysis, the demographic population of states having maximum immigration rate can be seen in the below image - \n",
    "  <img src=\"sqlqueryoutput.PNG\" width=\"600\"/>\n",
    "    \n",
    "    Steps involved in the process- \n",
    "    Step 1:- Scope the Project and Gather Data\n",
    "    Step 2:- Explore and Assess the Data\n",
    "    Step 3:- Define the Data Model\n",
    "    Step 4:- Run ETL to Model the Data\n",
    "    Step 5:- Future design consideration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope - \n",
    "    This project will integrate the immigration data, global temperature data and US demographic data to setup a data warehouse with fact and dimensions tables. We will use AWS redshift as data warehouse and we will model our data according to star schema.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "##### Datasets involved - \n",
    "    1. I94 immigration dataset - This data comes from the US National Tourism and Trade Office. The National Travel and Tourism Office (NTTO) works cooperatively with the U.S. Department of Homeland Security (DHS)/U.S. Customs and Border Protection (CBP) to release I-94 Visitor Arrivals Program data, providing a comprehensive count of all visitors (overseas all travel modes plus Mexico air and sea) entering the United States. The dataset can be found on the following link - https://www.trade.gov/travel-and-tourism-research\n",
    "    Dataset Format - SAS7BDAT\n",
    "    Number of records - 3096313\n",
    "\n",
    "    2. Global temperature dataset - The following dataset is availiable on Kaggle and is repackaged from a compilation put together by the Berkeley Earth, which is affiliated with Lawrence Berkeley National Laboratory. The Berkeley Earth Surface Temperature Study combines 1.6 billion temperature reports from 16 pre-existing archives.\n",
    "    Link to dataset - https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "    Data format - CSV\n",
    "    Number of records - 645675\n",
    "\n",
    "    3. U.S. City Demographic Data - This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. \n",
    "    This data comes from the US Census Bureau's 2015 American Community Survey.\n",
    "    Link to dataset - https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n",
    "    Data format - CSV\n",
    "    Number of records - 2891\n",
    "\n",
    "    4. Label dataset - At last we have a file \"I94_SAS_Labels_Descriptions.SAS\" which contains the state code, country code and city code.\n",
    "\n",
    "\n",
    "##### Tools used - \n",
    "    1. Python - Python is a high-level, general-purpose programming language. \n",
    "    2. AWS S3 - Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. We have used this to store the dataset in  parquet files.\n",
    "    3. AWS redshift - Amazon Redshift is a data warehouse product which forms part of the larger cloud-computing platform Amazon Web Services. It is built on top of technology from the massive parallel processing data warehouse company ParAccel, to handle large scale data sets and database migrations. We will use this to store the tables.\n",
    "    4. AWS EMR - Amazon EMR is the industry-leading cloud big data solution for petabyte-scale data processing, interactive analytics, and machine learning using open-source frameworks such as Apache Spark, Apache Hive, and Presto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "    Note - Make sure you have populated the credentials in the config.cfg file present in the aws folder, also make sure the tables are created on the redshift cluster by running the create_table.sql script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType, StringType, DoubleType, IntegerType, DateType\n",
    "from pyspark.sql.functions import udf, col, lit, year, month, upper, to_date\n",
    "from pyspark.sql.functions import monotonically_increasing_id, min, max, avg, sum, count\n",
    "import psycopg2\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('./aws/config.cfg'))\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "inputS3 = config['S3']['inputS3']\n",
    "outputS3 = config['S3']['outputS3']\n",
    "ARN=config['AWS']['ARN']\n",
    "HOST = config['CLUSTER']['HOST']\n",
    "DB = config['CLUSTER']['DB']\n",
    "DBUSER = config['CLUSTER']['DBUSER']\n",
    "DBPASSWORD = config['CLUSTER']['DBPASSWORD']\n",
    "DBPORT = config['CLUSTER']['DBPORT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "label_data =spark.read.text(inputS3+'I94_SAS_Labels_Descriptions.SAS').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_data = spark.read.format('com.github.saurfang.sas.spark').load(inputS3 + 'i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demography_data=spark.read.options(delimiter=';').options(inferSchema=True).csv(inputS3 + 'us-cities-demographics.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperature_data = spark.read.options(inferSchema=True).csv(inputS3 + 'GlobalLandTemperaturesByState.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Dataset 1 - Immigration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|       admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null|1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null| 3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS| 6.66643185E8|   93|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Steps to be performed -\n",
    "1. The arrival date and departure date format should be corrected\n",
    "2. Drop records which have airline and i94addr as null\n",
    "3. The residence country and citizen country are double, convert it to string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Dataset 2 - Label dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>libname library 'Your file location' ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>proc format library=library ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/* I94YR - 4 digit year */</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/* I94MON - Numeric month */</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/* I94CIT &amp; I94RES - This format shows all the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>value i94cntyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>582 =  'MEXICO Air Sea, and Not Reported (I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               value\n",
       "0             libname library 'Your file location' ;\n",
       "1                      proc format library=library ;\n",
       "2                                                   \n",
       "3                         /* I94YR - 4 digit year */\n",
       "4                                                   \n",
       "5                       /* I94MON - Numeric month */\n",
       "6                                                   \n",
       "7  /* I94CIT & I94RES - This format shows all the...\n",
       "8                                     value i94cntyl\n",
       "9     582 =  'MEXICO Air Sea, and Not Reported (I..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Steps to be performed -\n",
    "1. The country, city and  state codes will have to be extracted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Dataset 3 - Temperature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----------------------------+-----+-------+\n",
      "|                 dt|AverageTemperature|AverageTemperatureUncertainty|State|Country|\n",
      "+-------------------+------------------+-----------------------------+-----+-------+\n",
      "|1855-05-01 00:00:00|            25.544|                        1.171| Acre| Brazil|\n",
      "|1855-06-01 00:00:00|            24.228|                        1.103| Acre| Brazil|\n",
      "|1855-07-01 00:00:00|            24.371|                        1.044| Acre| Brazil|\n",
      "+-------------------+------------------+-----------------------------+-----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "645675"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Steps to be perfromed - \n",
    "1. Filter out country by USA\n",
    "2. Convert timestamp to date\n",
    "3. Replace state full names with state codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Dataset 4 - Demography dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|         City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|              Race|Count|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|Hispanic or Latino|25924|\n",
      "|       Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|             White|58723|\n",
      "|       Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|             Asian| 4759|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demography_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demography_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Steps to be perfromed - \n",
    "1. Tranform city, state in dimension table to upper case to match city_code and state_code table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "#### Cleaning Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. Process the immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def SAS_to_datetime(date):\n",
    "    if date == None:\n",
    "        return '1960-1-1'\n",
    "    return str(pd.to_timedelta(date, unit='D') + pd.Timestamp('1960-1-1').date())\n",
    "\n",
    "def double_to_str(num):\n",
    "    return str(int(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_immigration_data(immigration_data):\n",
    "    SAS_to_datetimeUDF = udf(lambda z: SAS_to_datetime(z),StringType())\n",
    "    double_to_strUDF = udf(lambda z: double_to_str(z),StringType())\n",
    "    immigration_data = immigration_data.withColumn('arrival_date',SAS_to_datetimeUDF(immigration_data['arrdate']))\n",
    "    immigration_data = immigration_data.withColumn('departure_date',SAS_to_datetimeUDF(immigration_data['depdate']))\n",
    "    immigration_data =immigration_data.withColumn('arrival_date', to_date(col('arrival_date'),'yyyy-MM-dd'))\n",
    "    immigration_data =immigration_data.withColumn('departure_date', to_date(col('departure_date'),'yyyy-MM-dd'))\n",
    "    immigration_data = immigration_data.drop('arrdate','depdate')\n",
    "    immigration_data = immigration_data.filter('i94addr is not NULL')\n",
    "    immigration_data = immigration_data.filter('airline is not NULL')\n",
    "    immigration_data = immigration_data.withColumn('i94cit', double_to_strUDF(col('i94cit')))\n",
    "    immigration_data = immigration_data.withColumn('i94res', double_to_strUDF(col('i94res')))\n",
    "    immigration_data = immigration_data.withColumn('i94visa', double_to_strUDF(col('i94visa')))\n",
    "    print(\"Immigration table after processing\")\n",
    "    immigration_data.show(3)\n",
    "    return immigration_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigration table after processing\n",
      "+-----+------+------+------+------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+------------+--------------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|i94mode|i94addr|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|arrival_date|departure_date|\n",
      "+-----+------+------+------+------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+------------+--------------+\n",
      "| 15.0|2016.0|   4.0|   101|   101|    WAS|    1.0|     MI|  55.0|      2|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|  2016-04-01|    2016-08-25|\n",
      "| 16.0|2016.0|   4.0|   101|   101|    NYC|    1.0|     MA|  28.0|      2|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|  2016-04-01|    2016-04-23|\n",
      "| 17.0|2016.0|   4.0|   101|   101|    NYC|    1.0|     MA|   4.0|      2|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|  2016-04-01|    2016-04-23|\n",
      "+-----+------+------+------+------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_immigration_data = clean_immigration_data(immigration_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "2. Process the Label file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_country_codeDF(label_data):\n",
    "    country_code = {}\n",
    "    for countries in label_data.value[9:298]:\n",
    "        pair = countries.split('=')\n",
    "        code, country = pair[0].strip(), pair[1].strip().strip(\"'\")\n",
    "        country_code[code] = country\n",
    "    return country_code\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_city_codeDF(label_data):\n",
    "    city_code = {}\n",
    "    for cities in label_data.value[303:962]:\n",
    "        pair = cities.split('=')\n",
    "        code, city = pair[0].strip(\"\\t\").strip().strip(\"'\"), pair[1].strip('\\t').strip().strip(\"''\")\n",
    "        city_code[code] = city\n",
    "    return city_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_state_codeDF(label_data):\n",
    "    state_code = {}\n",
    "    for states in label_data.value[982:1036]:\n",
    "        pair = states.split('=')\n",
    "        code, state = pair[0].strip('\\t').strip(\"'\"), pair[1].strip().strip(\"'\")\n",
    "        state_code[code] = state\n",
    "    return state_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_visa_codeDF(label_data):\n",
    "    visa_code = {}\n",
    "    for visa in label_data.value[1046:1049]:\n",
    "        pair = visa.split('=')\n",
    "        code, visa = pair[0].strip('\\t').strip(\"'\"), pair[1].strip().strip(\"'\")\n",
    "        visa_code[code] = visa\n",
    "    return visa_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_label_data(label_data):\n",
    "    country_code = create_country_codeDF(label_data)\n",
    "    city_code = create_city_codeDF(label_data)\n",
    "    state_code = create_state_codeDF(label_data)\n",
    "    visa_code = create_visa_codeDF(label_data)\n",
    "    country_code = spark.createDataFrame(country_code.items(), ['code', 'country'])\n",
    "    city_code = spark.createDataFrame(city_code.items(), ['code', 'city'])\n",
    "    state_code= spark.createDataFrame(state_code.items(), ['code', 'state'])\n",
    "    visa_code = spark.createDataFrame(visa_code.items(), ['code', 'visa'])\n",
    "    return country_code, city_code,state_code, visa_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_code, city_code,state_code, visa_code=clean_label_data(label_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "3. Process demography data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_demography_data(demography_data):\n",
    "    upper_case_transform = udf(lambda z: str(z).upper(),StringType())\n",
    "    demography_data =demography_data.withColumn('City',upper_case_transform(demography_data['City']))\n",
    "    demography_data =demography_data.withColumn('State',upper_case_transform(demography_data['State'])) \n",
    "    print(\"Demography table after processing -\")\n",
    "    demography_data.show(3)\n",
    "    return demography_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demography table after processing -\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|         City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|              Race|Count|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|SILVER SPRING|     MARYLAND|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|Hispanic or Latino|25924|\n",
      "|       QUINCY|MASSACHUSETTS|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|             White|58723|\n",
      "|       HOOVER|      ALABAMA|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|             Asian| 4759|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_demography_data = clean_demography_data(demography_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "4. Process temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_temperature_data(temperature_data, state_code): \n",
    "    upper_case_transform = udf(lambda z: str(z).upper(),StringType())\n",
    "    temperature_data = temperature_data.filter(col('Country') == 'United States')\n",
    "    temperature_data = temperature_data.withColumn('dt', to_date('dt'))\n",
    "    temperature_data = temperature_data.withColumn('State', upper_case_transform(col('State')))\n",
    "    temperature_data = temperature_data.na.drop(subset=['AverageTemperature'])\n",
    "    temperature_data = temperature_data.join(state_code, state_code.state == temperature_data.State)\n",
    "    temperature_data = temperature_data.select('dt', 'AverageTemperature', 'AverageTemperatureUncertainty', 'code', 'Country')\n",
    "    print(\"temperature table after processing -\")\n",
    "    temperature_data.show(5)\n",
    "    return temperature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature table after processing -\n",
      "+----------+------------------+-----------------------------+----+-------------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|code|      Country|\n",
      "+----------+------------------+-----------------------------+----+-------------+\n",
      "|1743-11-01|             4.597|                         1.99|  NJ|United States|\n",
      "|1744-04-01|            10.675|                        2.406|  NJ|United States|\n",
      "|1744-05-01|            16.271|                        1.912|  NJ|United States|\n",
      "|1744-06-01|            21.755|                        1.947|  NJ|United States|\n",
      "|1744-07-01|22.881999999999998|                        1.865|  NJ|United States|\n",
      "+----------+------------------+-----------------------------+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temperature = clean_temperature_data(temperature_data, state_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.1 Conceptual Data Model\n",
    "The data model and data dictionary is present in the excel sheet, name of sheet - model.xlsx\n",
    "For this project we will be using star schema, we are using the fact table and dimension tables as below -\n",
    "Fact table - fact_immigration\n",
    "Dimension tables -  dim_city_population, dim_state_population, country_code, state_code, city_code, visa_code, dim_temperature\n",
    "The reason we chose star schema is because it is simple to design and it is easy to derieve business insight from this model.\n",
    "\n",
    "\n",
    "Description of our data model - \n",
    "\n",
    "Fact_immigration is our fact table, it has the actual immigration records. The fact table have fields city_code, state_code, country_code and visa_code which serve as a reference to the city_code, state_code, country_code and visa_code tables. For connectivity between the demography dataset we had to first group the demography dataset by state and then aggregate the parameters. Later the state_code of the fact_immigration table referenced to the state_code of the dim_state_population table. Now to retreive the city level data would not have been possible as our label dataset did not had all the cities of the demography table so we have referenced the dim_city_population table to dim_state_population by state_code. The temperature dataset dimension(dim_temperature) was at the state level so we used a composite primary key(dt,code) was used and the fact_immigration table referenced to it by arrive_date and state_code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src=\"data_model.png\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "1. Assume all data is stored in S3 bucket as below - \n",
    "    - s3://bucket/Data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n",
    "    - s3://bucket/Data/I94_SAS_Labels_Descriptions.SAS\n",
    "    - s3://bucket/Data/GlobalLandTemperaturesByState.csv\n",
    "    - s3://bucket/Data/us-cities-demographics.csv\n",
    "2. Read the files from the s3 bucket into the EMR cluster\n",
    "3. Perform data cleaning\n",
    "4. Create dataframes with required columns\n",
    "5. Write tables in parquet format to destination S3 bucket\n",
    "6. Read parquet data and perform data quality checks\n",
    "7. Load the data to redshift cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Immigration dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create immigration fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_immi_fact_table(processed_immigration_data):\n",
    "    fact_immigration = processed_immigration_data.select('cicid', 'i94yr', 'i94mon', 'i94port', 'i94addr',\\\n",
    "                                 'arrival_date', 'departure_date', 'i94visa', 'visatype', 'i94cit','biryear', 'gender', 'airline', 'admnum').distinct()\n",
    "    fact_immigration = fact_immigration.toDF('cic_id', 'year', 'month', 'city_code', 'state_code',\\\n",
    "                   'arrive_date', 'departure_date', 'visa_code', 'visatype', 'citizen_country','birthyear', 'gender', 'airline', 'admitnum')\n",
    "    fact_immigration = fact_immigration.withColumn('country_immigrated_to', lit('United States'))     \n",
    "    print(\"Created fact_immigration table as below -\")   \n",
    "    fact_immigration.show(5)            \n",
    "    return fact_immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fact_immigration table as below -\n",
      "+------+------+-----+---------+----------+-----------+--------------+---------+--------+---------------+---------+------+-------+---------------+---------------------+\n",
      "|cic_id|  year|month|city_code|state_code|arrive_date|departure_date|visa_code|visatype|citizen_country|birthyear|gender|airline|       admitnum|country_immigrated_to|\n",
      "+------+------+-----+---------+----------+-----------+--------------+---------+--------+---------------+---------+------+-------+---------------+---------------------+\n",
      "| 294.0|2016.0|  4.0|      NYC|        NY| 2016-04-01|    2016-04-02|        2|      WT|            103|   1991.0|  null|     DL|5.5450935033E10|        United States|\n",
      "| 739.0|2016.0|  4.0|      FTL|        FL| 2016-04-01|    2016-04-02|        2|      WT|            103|   2007.0|     M|     NK|5.5453752933E10|        United States|\n",
      "| 827.0|2016.0|  4.0|      BOS|        MA| 2016-04-01|    2016-04-10|        2|      WT|            104|   1989.0|     M|     DL|5.5416479333E10|        United States|\n",
      "|1295.0|2016.0|  4.0|      NYC|        NY| 2016-04-01|    2016-04-05|        2|      WT|            104|   1968.0|     M|     AF|5.5443983033E10|        United States|\n",
      "|1602.0|2016.0|  4.0|      LOS|        NV| 2016-04-01|    2016-04-09|        2|      WT|            104|   1978.0|     M|     LH|5.5434656933E10|        United States|\n",
      "+------+------+-----+---------+----------+-----------+--------------+---------+--------+---------------+---------+------+-------+---------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigration = create_immi_fact_table(processed_immigration_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_immigration.write.mode(\"overwrite\").parquet(outputS3 + 'fact_immigration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Label dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create Country code dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_code.write.mode(\"overwrite\").parquet(outputS3+'country_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create City code dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "city_code.write.mode(\"overwrite\").parquet(outputS3 +'city_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create State code dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_code.write.mode(\"overwrite\").parquet(outputS3+ 'state_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create visa code dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "visa_code.write.mode(\"overwrite\").parquet(outputS3+ 'visa_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demography data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_dim_demog_population(processed_demography_data):\n",
    "    dim_demog_population = processed_demography_data.select(['City', 'State Code','Male Population', 'Female Population',\\\n",
    "                              'Number of Veterans', 'Foreign-born', 'Total Population', 'Average Household Size',  'Median Age']).distinct()\n",
    "    dim_demog_population = dim_demog_population.toDF('city', 'State_Code', 'male_population', 'female_population',\\\n",
    "                               'num_vetarans', 'foreign_born', 'total_population', 'AvgHouseholdSize', 'Median_Age')\n",
    "    dim_city_population = dim_demog_population\n",
    "    dim_state_population = dim_demog_population.groupBy('State_Code') \\\n",
    "                    .agg(\\\n",
    "                        sum('male_population').alias('male_population'),\\\n",
    "                        sum('female_population').alias('female_population'),\\\n",
    "                        sum('num_vetarans').alias('num_vetarans'),\\\n",
    "                        sum('foreign_born').alias('foreign_born'),\\\n",
    "                        sum('total_population').alias('total_population'),\\\n",
    "                        avg('AvgHouseholdSize').alias('AvgHouseholdSize'),\\\n",
    "                        avg('Median_Age').alias('Median_Age')\n",
    "                        )\n",
    "    print(\"Created dim_state_population as below - \")                               \n",
    "    dim_state_population.show(5)\n",
    "    print(\"Created dim_city_population as below - \")\n",
    "    dim_city_population.show(5)\n",
    "    return dim_state_population, dim_city_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dim_state_population as below - \n",
      "+----------+---------------+-----------------+------------+------------+----------------+------------------+-----------------+\n",
      "|State_Code|male_population|female_population|num_vetarans|foreign_born|total_population|  AvgHouseholdSize|       Median_Age|\n",
      "+----------+---------------+-----------------+------------+------------+----------------+------------------+-----------------+\n",
      "|        AZ|        2227455|          2272087|      264505|      682313|         4499542|          2.774375|          35.0375|\n",
      "|        SC|         260944|           272713|       33463|       27744|          533657|             2.472|34.17999999999999|\n",
      "|        LA|         626998|           673597|       69771|       83419|         1300595|             2.465|           34.625|\n",
      "|        MN|         702157|           720246|       64894|      215873|         1422403|2.5009090909090914|35.61818181818182|\n",
      "|        NJ|         705736|           723172|       30195|      477028|         1428908|2.9658333333333338|           35.125|\n",
      "+----------+---------------+-----------------+------------+------------+----------------+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Created dim_city_population as below - \n",
      "+-----------+----------+---------------+-----------------+------------+------------+----------------+----------------+----------+\n",
      "|       city|State_Code|male_population|female_population|num_vetarans|foreign_born|total_population|AvgHouseholdSize|Median_Age|\n",
      "+-----------+----------+---------------+-----------------+------------+------------+----------------+----------------+----------+\n",
      "|JOHNS CREEK|        GA|          40282|            43057|        1983|       29000|           83339|            3.19|      39.0|\n",
      "| PLANTATION|        FL|          42881|            49674|        4037|       27902|           92555|            2.73|      40.6|\n",
      "|   CARLSBAD|        CA|          55119|            58347|        6031|       17689|          113466|            2.68|      42.1|\n",
      "|CHATTANOOGA|        TN|          83640|            92957|       10001|       10599|          176597|             2.4|      36.6|\n",
      "|      TYLER|        TX|          50422|            53283|        4813|        8225|          103705|            2.59|      33.9|\n",
      "+-----------+----------+---------------+-----------------+------------+------------+----------------+----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_state_population, dim_city_population = create_dim_demog_population(processed_demography_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_city_population.write.mode(\"overwrite\")\\\n",
    "                        .parquet(outputS3 + 'dim_city_population')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_state_population.write.mode(\"overwrite\")\\\n",
    "                        .parquet(outputS3 + 'dim_state_population')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+----+-------------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|code|      Country|\n",
      "+----------+------------------+-----------------------------+----+-------------+\n",
      "|1743-11-01|             4.597|                         1.99|  NJ|United States|\n",
      "|1744-04-01|            10.675|                        2.406|  NJ|United States|\n",
      "|1744-05-01|            16.271|                        1.912|  NJ|United States|\n",
      "|1744-06-01|            21.755|                        1.947|  NJ|United States|\n",
      "|1744-07-01|22.881999999999998|                        1.865|  NJ|United States|\n",
      "+----------+------------------+-----------------------------+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temperature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_temperature.write.parquet(outputS3 + 'dim_temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Make sure the tables are not empty and columns does not have any null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "table_DF = [\n",
    "        {\n",
    "            'table_name': 'fact_immigration',\n",
    "            'columns': ['cic_id', 'arrive_date', 'city_code', 'state_code'],\n",
    "            'expected_result': 0\n",
    "        },\n",
    "        {\n",
    "            'table_name': 'dim_state_population',\n",
    "            'columns': ['State_Code'],\n",
    "            'expected_result': 0\n",
    "        },\n",
    "        {\n",
    "            'table_name': 'dim_temperature',\n",
    "            'columns': ['AverageTemperature','dt'],\n",
    "            'expected_result': 0\n",
    "        },\n",
    "        {\n",
    "            'table_name': 'country_code',\n",
    "            'columns': ['country'],\n",
    "            'expected_result': 0\n",
    "        },\n",
    "        {\n",
    "            'table_name': 'city_code',\n",
    "            'columns': ['city'],\n",
    "            'expected_result': 0\n",
    "        },\n",
    "        {\n",
    "            'table_name': 'state_code',\n",
    "            'columns': ['state'],\n",
    "            'expected_result': 0\n",
    "        },\n",
    "        {\n",
    "            'table_name': 'visa_code',\n",
    "            'columns': ['visa'],\n",
    "            'expected_result': 0\n",
    "        },\n",
    "        {\n",
    "            'table_name': 'dim_city_population',\n",
    "            'columns': ['city', 'State_Code'],\n",
    "            'expected_result': 0\n",
    "        }\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def perform_data_quality_check(spark , DF, outputS3):\n",
    "    '''\n",
    "    Processes individual test case, takes spark session object, an individual test case and location of created table as input\n",
    "    '''\n",
    "    table_name = DF['table_name']\n",
    "    columnlist = DF['columns']\n",
    "    expected_result = DF['expected_result']\n",
    "    query = ' is NULL OR '.join(columnlist) + ' is NULL'   \n",
    "    \n",
    "    parquettable = spark.read.parquet(outputS3 + table_name)\n",
    "    if(parquettable.count() >0):\n",
    "        print( table_name+\" table is not empty\")\n",
    "    else:\n",
    "        print( table_name+\" table not empty\")\n",
    "        \n",
    "    parquettable_count = parquettable.filter(query).count()\n",
    "    if parquettable_count == 0:\n",
    "        print('Data quality check suceeded for {} table'.format(table_name))    \n",
    "    else:\n",
    "        print('Data quality check failed for {} table'.format(table_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_quality_check(spark, outputS3, table_DF):\n",
    "    '''\n",
    "        Perform the data quality check on the data present at the output S3 bucket, takes the spark session\n",
    "        object and the destination S3 path as arguments.\n",
    "    '''\n",
    "    for i in table_DF:\n",
    "        perform_data_quality_check(spark, i, outputS3)\n",
    "    print(\"Checks complete for s3 staged data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_immigration table is not empty\n",
      "Data quality check suceeded for fact_immigration table\n",
      "dim_state_population table is not empty\n",
      "Data quality check suceeded for dim_state_population table\n",
      "dim_temperature table is not empty\n",
      "Data quality check suceeded for dim_temperature table\n",
      "country_code table is not empty\n",
      "Data quality check suceeded for country_code table\n",
      "city_code table is not empty\n",
      "Data quality check suceeded for city_code table\n",
      "state_code table is not empty\n",
      "Data quality check suceeded for state_code table\n",
      "visa_code table is not empty\n",
      "Data quality check suceeded for visa_code table\n",
      "dim_city_population table is not empty\n",
      "Data quality check suceeded for dim_city_population table\n",
      "Checks complete for s3 staged data\n"
     ]
    }
   ],
   "source": [
    "data_quality_check(spark,outputS3, table_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_connection_to_redshift():\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(HOST,DB,DBUSER,DBPASSWORD,DBPORT))\n",
    "    cur = conn.cursor()\n",
    "    return conn,cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def insert_data_into_redshift(conn, cur):\n",
    "    query_template =\"COPY {} FROM '{}{}' IAM_ROLE '{}' FORMAT AS PARQUET;\"\n",
    "    table_names= ['fact_immigration', 'dim_city_population','dim_state_population',\\\n",
    "                        'dim_temperature', 'country_code', 'city_code','state_code', 'visa_code']\n",
    "    for i in table_names:\n",
    "        cur.execute(query_template.format(i,outputS3,i,ARN))\n",
    "        conn.commit()\n",
    "        print(\"Data loaded in table \" + i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn, cur = get_connection_to_redshift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in table fact_immigration\n",
      "Data loaded in table dim_city_population\n",
      "Data loaded in table dim_state_population\n",
      "Data loaded in table dim_temperature\n",
      "Data loaded in table country_code\n",
      "Data loaded in table city_code\n",
      "Data loaded in table state_code\n",
      "Data loaded in table visa_code\n"
     ]
    }
   ],
   "source": [
    "insert_data_into_redshift(conn, cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Fetch statistics of top 5 states which had maximum immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "query_top5states_immigration=\"\"\"SELECT * from\n",
    "                        (\n",
    "                        SELECT \n",
    "                            immigration.state_code,\n",
    "                            COUNT(cic_id) as visitor_count\n",
    "                        FROM \"dev\".\"public\".\"fact_immigration\" as immigration\n",
    "                        GROUP BY immigration.state_code\n",
    "                        ORDER BY COUNT(cic_id) DESC\n",
    "                        ) as demo, dim_state_population\n",
    "                        WHERE \n",
    "                            dim_state_population.state_code = demo.state_code\n",
    "                        ORDER BY demo.visitor_count DESC\n",
    "                        LIMIT 5;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cur.execute(query_top5states_immigration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "rows = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('FL', 187, 'FL', 3236773, 3487375, 388228, 1688931, 6796738, 2.77787234042553, 39.6666666666667)\n",
      "('CA', 159, 'CA', 12278281, 12544179, 928270, 7448257, 24822460, 3.10094890510949, 36.1824817518248)\n",
      "('NY', 157, 'NY', 4692055, 5123571, 204901, 3438081, 9815626, 2.76181818181818, 35.6636363636364)\n",
      "('HI', 52, 'HI', 176807, 175959, 23213, 101312, 352766, 2.69, 41.4)\n",
      "('TX', 39, 'TX', 7063571, 7236412, 693501, 2942164, 14299983, 2.86, 33.340350877193)\n"
     ]
    }
   ],
   "source": [
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "The data model and data dictionary is present in the excel sheet, name of sheet - model.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5 - Future design consideration\n",
    "    A data pipeline with airflow can be setup to automatically read, process and load the data from s3 to redshift, the pipeline can be either triggerred manually or be scheduled at a specific time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Project Write Up\n",
    "##### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "    1. Python - Python is a high-level, general-purpose programming language. It has manay useful packages namely pandas, pyspark which we have used in this project to process the data.\n",
    "    2. AWS S3 - Amazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. We have used this to store the dataset in  parquet files.\n",
    "    3. AWS redshift - Amazon Redshift is a data warehouse product which forms part of the larger cloud-computing platform Amazon Web Services. It is built on top of technology from the massive parallel processing data warehouse company ParAccel, to handle large scale data sets and database migrations. We will use this to store the tables.\n",
    "    4. AWS EMR - Amazon Elastic Map Reduce is a web service that you can use to process large amounts of data efficiently. Amazon EMR uses Hadoop processing combined with several AWS products to do such tasks as web indexing, data mining, log file analysis, machine learning, scientific simulation, and data warehousing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Propose how often the data should be updated and why.\n",
    "    - The fact and dimension table of immigration dataset should be updated every month, temperature dataset can be updated on biweekly or monthly while the demography data should be updated once per year as it will take time to gather the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Write a description of how you would approach the problem differently under the following scenarios:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### The data was increased by 100x.\n",
    "    - The use of Redshift perfectly handles this situation in case of data warehousing, in terms of processing the data the EMR cluster can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    - The data pipeline solution like apache airflow can we used to schedule the run before 7am everyday.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### The database needed to be accessed by 100+ people.\n",
    "    - The default value for mximum number of connections handled by redshift is 500, so use of redshift handles this requirement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
